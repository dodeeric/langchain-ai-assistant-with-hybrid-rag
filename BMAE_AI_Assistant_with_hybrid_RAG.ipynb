{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RftTGaPRQ4WlLUuHd-cNCDW65B1wUgjM",
      "authorship_tag": "ABX9TyOL4wijb0Fa4YePbx9FhUU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dodeeric/langchain-ai-assistant-with-hybrid-rag/blob/main/BMAE_AI_Assistant_with_hybrid_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI Assistant (LLM Chatbot) with Hybrid RAG\n",
        "Hybrid RAG: keyword search (bm25) and semantic search (vector db)"
      ],
      "metadata": {
        "id": "uN1I_30g_aFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scape Web pages and save the result in JSON file"
      ],
      "metadata": {
        "id": "tm_zLVsh-276"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kei890_wuANp"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet bs4\n",
        "\n",
        "import requests, json, time\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuDtjV1WyfyR"
      },
      "outputs": [],
      "source": [
        "# Function: Scrape Commons Summary = scs\n",
        "# Scrape the summary section and the metadata fields of a Wikimedia Commons web page.\n",
        "\n",
        "def scrape_commons_summary(url):\n",
        "    \"\"\"\n",
        "    Scrape the summary section and the metadata fields of a Wikimedia Commons web page.\n",
        "    Input: URL of the page\n",
        "    Output: JSON with: url: url, metadata: metadata, text: summary text\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the HTML code\n",
        "    response = requests.get(url)\n",
        "    # Transform the HTML code from a Response object type into a BeautifulSoup object type to be scraped by Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Get the Summary content which is in a specific table\n",
        "    content_table = soup.find('div', {'class': 'hproduct commons-file-information-table'})\n",
        "    # The actual summary text is within 'td' tags\n",
        "    summary_cells = content_table.find_all('td')\n",
        "    # Extract the text from each cell. The \\n is needed to avoid (key-value): v1 k2\\n v2 k3\\n v3 k4 ==> k1\\n v1\\n k2\\n v2\\n\n",
        "    summary = '\\n'.join([td.get_text() for td in summary_cells])\n",
        "    # Replace '\\n\\n' by '\\n' (remove empty lines):\n",
        "    while '\\n\\n' in summary:\n",
        "        summary = summary.replace('\\n\\n', '\\n')\n",
        "\n",
        "    # Get the metadata fields\n",
        "    metadata = {} # Empty dictionary\n",
        "    # Find all the meta tags in the HTML\n",
        "    meta_tags = soup.find_all(\"meta\")\n",
        "    # Loop through the meta tags\n",
        "    for tag in meta_tags:\n",
        "        property = tag.get(\"property\")\n",
        "        content = tag.get(\"content\")\n",
        "        # Add the property-content pair to the dictionary\n",
        "        if property and content:\n",
        "            metadata[property] = content\n",
        "\n",
        "    # Build JSON string with: url: url, metadata: metadata, text: summary text\n",
        "    # Create a dictionary\n",
        "    page = {\n",
        "        \"url\": url, # String\n",
        "        \"metadata\": metadata, # Dictionary\n",
        "        \"text\": summary # String\n",
        "    }\n",
        "    # Convert the dictionary to a JSON string\n",
        "    page_json = json.dumps(page)\n",
        "    # Convert in clear text (convert codes in text)\n",
        "    #page_json_clear_text = page_json.encode('utf-8').decode('unicode_escape')\n",
        "\n",
        "    return page_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX4Rr4Lr8-2M"
      },
      "outputs": [],
      "source": [
        "# Scrape the URLs and save the results in a Python list\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/commons-urls-ds1\"\n",
        "\n",
        "with open(f\"{file_path}.txt\", \"r\") as url_file:\n",
        "    data = []\n",
        "    for line in url_file:\n",
        "        url = line.strip()\n",
        "        url = url.replace(\"\\ufeff\", \"\")  # Remove BOM\n",
        "        page_json = scrape_commons_summary(url)\n",
        "        data.append(page_json)\n",
        "        time.sleep(1)\n",
        "\n",
        "# Save the Python list in a JSON file. File name: xxx.txt --> xxx-scs.json\n",
        "with open(f\"{file_path}-scs.json\", \"w\") as output_file:\n",
        "        json.dump(data, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXt8lI13j5JA"
      },
      "outputs": [],
      "source": [
        "# Open the JSON file to check its content (will produce an error if it's not a correctly formated JSON file)\n",
        "with open(\"/content/drive/MyDrive/colab/commons-urls-ds1-scs.json\", \"r\") as file:\n",
        "    data_read = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index"
      ],
      "metadata": {
        "id": "nMRAdR5K-uQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the JSON file and embed the items in a Chroma vector DB."
      ],
      "metadata": {
        "id": "sPs_aAdrL4lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet jq langchain langchain-community langchain-openai langchain-chroma langchainhub rank_bm25\n",
        "\n",
        "import jq\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "%env OPENAI_API_KEY = $OPENAI_API_KEY\n",
        "%env LANGCHAIN_API_KEY = $LANGCHAIN_API_KEY\n",
        "%env LANGCHAIN_TRACING_V2 = \"true\""
      ],
      "metadata": {
        "id": "5OY-ZM_lxM-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the JSON file and parse/embed each item one by one\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/commons-urls-ds1-scs.json\"\n",
        "collection_name = \"bmae-json\"\n",
        "\n",
        "loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
        "documents = loader.load() # Chunks (JSON item) from the JSON file\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\") # 1536 dimenssions vectors used to embed the json items and the questions\n",
        "\n",
        "vector_db = Chroma.from_documents(documents, embedding_model, collection_name=collection_name, persist_directory=\"/content/drive/MyDrive/colab/chromadb2\")"
      ],
      "metadata": {
        "id": "PR-uONuCr7iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve and generate"
      ],
      "metadata": {
        "id": "Ku0gOfeiro_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM chatbot with a hybrid RAG chain:\n",
        "# (To embed the question, the same model is used as for the data; the model is given in \"vector_db\".)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", temperature=0.1)\n",
        "\n",
        "# Semantic search (vector retriever)\n",
        "vector_retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6}) # Chroma DB\n",
        "\n",
        "# Keyword search (bm25 retriever)\n",
        "keyword_retriever = BM25Retriever.from_documents(documents)\n",
        "keyword_retriever.k = 6\n",
        "\n",
        "# Ensemble retriever (mix of both retrivers)\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[keyword_retriever, vector_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "# Download prompt template (system prompt + context (rag documents) + user question)\n",
        "prompt = hub.pull(\"dodeeric/rag-prompt-bmae\")\n",
        "\n",
        "# Take the text content of each doc, and concatenate them in one string to pass to the prompt (context)\n",
        "def format_docs_clear_text(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content.encode('utf-8').decode('unicode_escape') for doc in docs)\n",
        "\n",
        "# Function to display the text content of the prompt in ai_assistant_chain\n",
        "def print_and_pass(data):\n",
        "    print(f\"Prompt content sent to the LLM: {data}\")\n",
        "    return data\n",
        "\n",
        "ai_assistant_chain = (\n",
        "    {\"context\": ensemble_retriever | format_docs_clear_text, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    #| print_and_pass\n",
        "    | llm\n",
        "    | StrOutputParser() # Convert to string\n",
        ")"
      ],
      "metadata": {
        "id": "EmX4A5jay8WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Querry the AI Assistant:"
      ],
      "metadata": {
        "id": "P_Vs5q4pAiWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Pouvez-vous me montrer des portraits du roi LÃ©opold Ier ?\""
      ],
      "metadata": {
        "id": "5iMV6D16T5y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_assistant_chain.invoke(question)"
      ],
      "metadata": {
        "id": "djaCokNX46hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the vector RAG only\n",
        "docs = vector_db.similarity_search(question, k=2) # List of Documents; page_content of a Document: string\n",
        "rag_context = format_docs_clear_text(docs) # One string composed of k json items\n",
        "print(rag_context)"
      ],
      "metadata": {
        "id": "qrFCZTwc1sMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}